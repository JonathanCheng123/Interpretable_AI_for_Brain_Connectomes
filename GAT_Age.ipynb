{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27481,
     "status": "ok",
     "timestamp": 1765149454447,
     "user": {
      "displayName": "Alexander Nguyen",
      "userId": "03900039906772620645"
     },
     "user_tz": 480
    },
    "id": "BHxDF94kJ43G",
    "outputId": "bba9f719-5521-4804-c4ae-20608fea0799"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 195336,
     "status": "ok",
     "timestamp": 1765149649788,
     "user": {
      "displayName": "Alexander Nguyen",
      "userId": "03900039906772620645"
     },
     "user_tz": 480
    },
    "id": "2UIV0E9UKGgu",
    "outputId": "04a3b6e9-991c-4a92-aab0-b018cc46ce84"
   },
   "outputs": [],
   "source": [
    "# Enable quiet mode to hide verbose output\n",
    "!pip install -q nilearn\n",
    "\n",
    "# Ensure a stable PyTorch version\n",
    "!pip install torch==2.5.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Install compatible PyG wheels for torch 2.5.1 + cu121\n",
    "!pip install torch-scatter==2.1.2 -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
    "!pip install torch-sparse==0.6.18 -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
    "!pip install torch-cluster==1.6.3 -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
    "!pip install torch-spline-conv==1.2.2 -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
    "!pip install torch-geometric==2.6.1\n",
    "\n",
    "# Check installations\n",
    "import torch, nilearn\n",
    "import torch_geometric\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Nilearn:\", nilearn.__version__)\n",
    "print(\"PyG:\", torch_geometric.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12909,
     "status": "ok",
     "timestamp": 1765149662702,
     "user": {
      "displayName": "Alexander Nguyen",
      "userId": "03900039906772620645"
     },
     "user_tz": 480
    },
    "id": "5A77NUvCKLyH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Paths\n",
    "data_dir = '/content/drive/Shareddrives/291A_Brain_Connectomes/AABC_Release1_Non-imaging_Data-XL.csv'\n",
    "cov_dir = '/content/drive/Shareddrives/291A_Brain_Connectomes/FullCorrelationSubjects'\n",
    "\n",
    "# Load phenotypic data\n",
    "phen_df = pd.read_csv(data_dir, low_memory=False)\n",
    "subject_ids = phen_df.iloc[1:, 0].tolist()\n",
    "\n",
    "# Pre-list files in the folder\n",
    "existing_files = set(os.listdir(cov_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 182466,
     "status": "ok",
     "timestamp": 1765149845343,
     "user": {
      "displayName": "Alexander Nguyen",
      "userId": "03900039906772620645"
     },
     "user_tz": 480
    },
    "id": "d2WzaWrKLSOk",
    "outputId": "ad0f7c9a-1654-4dab-967c-2be15e58bccc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cov_dict = {}\n",
    "pheno_dict = {}\n",
    "count = 0\n",
    "max_subjects = 500  # TODO: change later\n",
    "\n",
    "def load_covariance_numeric(path):\n",
    "    \"\"\"\n",
    "    Load a CSV covariance/correlation matrix and return only the numeric part.\n",
    "    Assumes first row and first column are labels.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    numeric_matrix = df.iloc[1:, 1:].to_numpy(dtype=float)  # skip first row/column\n",
    "    return numeric_matrix\n",
    "\n",
    "for subject_id in subject_ids:\n",
    "    filename = f\"{subject_id}.csv\"\n",
    "\n",
    "    if filename in existing_files:\n",
    "        # Load numeric covariance matrix\n",
    "        cov_matrix = load_covariance_numeric(os.path.join(cov_dir, filename))\n",
    "\n",
    "        # Load phenotypic row\n",
    "        row = phen_df.loc[phen_df.iloc[:, 0] == subject_id]\n",
    "        if row.empty:\n",
    "            continue  # skip if no phenotypic info\n",
    "        row_dict = row.iloc[0].to_dict()\n",
    "\n",
    "        # Add to dicts\n",
    "        cov_dict[subject_id] = cov_matrix\n",
    "        pheno_dict[subject_id] = row_dict\n",
    "\n",
    "        count += 1\n",
    "        if count >= max_subjects:\n",
    "            break\n",
    "\n",
    "print(\"Loaded subjects:\", len(cov_dict))\n",
    "first_id = list(cov_dict.keys())[0]\n",
    "print(\"Example subject:\", first_id)\n",
    "print(\"Covariance shape:\", cov_dict[first_id].shape)\n",
    "print(\"Phenotype keys:\", list(pheno_dict[first_id].keys())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1201,
     "status": "ok",
     "timestamp": 1765149846541,
     "user": {
      "displayName": "Alexander Nguyen",
      "userId": "03900039906772620645"
     },
     "user_tz": 480
    },
    "id": "aBqbqz9kSmoQ",
    "outputId": "26d04543-a561-402f-d061-4136ce6584db"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "\n",
    "age_col = \"age_open - Age in years, truncated at 90.  Note that this will result in some individuals appearing to be the same age (e.g. 90) for most or all of the visits that happened several years apart.\"\n",
    "\n",
    "def clean_age(age_val):\n",
    "    if isinstance(age_val, str) and '90' in age_val:\n",
    "        return 90.0\n",
    "    return float(age_val)\n",
    "\n",
    "dataset = []\n",
    "threshold = 0.2\n",
    "\n",
    "for subj_id, cov_matrix in cov_dict.items():\n",
    "\n",
    "    # Skip missing ages\n",
    "    if age_col not in pheno_dict[subj_id]:\n",
    "        continue\n",
    "    age_value = clean_age(pheno_dict[subj_id][age_col])\n",
    "    if np.isnan(age_value):\n",
    "        continue\n",
    "\n",
    "    y = torch.tensor([age_value], dtype=torch.float)\n",
    "\n",
    "    # convert and symmetrize\n",
    "    cov_matrix = np.array(cov_matrix, dtype=float)\n",
    "    cov_matrix = (cov_matrix + cov_matrix.T) / 2\n",
    "\n",
    "    # Threshold\n",
    "    cov_matrix[np.abs(cov_matrix) < threshold] = 0\n",
    "\n",
    "    # Node features = full row vector for each node\n",
    "    x = torch.tensor(cov_matrix, dtype=torch.float)\n",
    "\n",
    "    # Edges (nonzero)\n",
    "    row, col = np.nonzero(cov_matrix)\n",
    "    edge_index = torch.tensor(np.vstack((row, col)), dtype=torch.long)\n",
    "\n",
    "    # Edge weights from correlation/covariance values\n",
    "    edge_attr = torch.tensor(cov_matrix[row, col], dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "    data = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=y,\n",
    "        sub_id=subj_id\n",
    "    )\n",
    "\n",
    "    dataset.append(data)\n",
    "\n",
    "print(f\"Prepared {len(dataset)} subjects for PyG GNN.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1765149846893,
     "user": {
      "displayName": "Alexander Nguyen",
      "userId": "03900039906772620645"
     },
     "user_tz": 480
    },
    "id": "vRZAdNf6VKeW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Model definition\n",
    "class GATRegressor(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=8, heads=2):\n",
    "        super().__init__()\n",
    "        self.gat1 = GATConv(in_dim, hidden_dim, heads=heads, concat=True, dropout=0.4, add_self_loops=False)\n",
    "        self.gat2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=0.4, add_self_loops=False)\n",
    "        self.gat3 = GATConv(hidden_dim * heads, 1, heads=1, concat=False, dropout=0.4, add_self_loops=False)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.elu(self.gat1(x, edge_index))\n",
    "        x = F.elu(self.gat2(x, edge_index))\n",
    "        x = self.gat3(x, edge_index)\n",
    "        return global_mean_pool(x, batch).view(-1)\n",
    "\n",
    "# Scale ages\n",
    "ages_raw = torch.tensor([d.y.item() for d in dataset], dtype=torch.float).view(-1, 1)\n",
    "scaler = StandardScaler()\n",
    "ages_scaled = torch.tensor(scaler.fit_transform(ages_raw), dtype=torch.float)\n",
    "\n",
    "for i, d in enumerate(dataset):\n",
    "    d.y = ages_scaled[i]  # replace y with scaled value\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Model, optimizer, loss\n",
    "in_dim = dataset[0].x.shape[1]\n",
    "model = GATRegressor(in_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 230261,
     "status": "ok",
     "timestamp": 1765150077156,
     "user": {
      "displayName": "Alexander Nguyen",
      "userId": "03900039906772620645"
     },
     "user_tz": 480
    },
    "id": "4lsR3WsQVb6t",
    "outputId": "f1c152cf-e8a5-4075-c489-4d0fd2863fef"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(batch)\n",
    "        loss = criterion(pred, batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1:03d} - Train Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1765150077637,
     "user": {
      "displayName": "Alexander Nguyen",
      "userId": "03900039906772620645"
     },
     "user_tz": 480
    },
    "id": "zMVgOZeQVgmZ",
    "outputId": "454edf94-7f38-4320-9cd5-5d2ecbadbb42"
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        pred_scaled = model(batch)\n",
    "        # convert back to real age\n",
    "        pred_np = pred_scaled.numpy().reshape(-1, 1)\n",
    "        pred_real = scaler.inverse_transform(pred_np).flatten()\n",
    "        actual_np = batch.y.numpy().reshape(-1, 1)\n",
    "        actual_real = scaler.inverse_transform(actual_np).flatten()\n",
    "\n",
    "        for sub_id, p_val, a_val in zip(batch.sub_id, pred_real, actual_real):\n",
    "            preds.append((sub_id, float(p_val), float(a_val)))\n",
    "            print(f\"{sub_id}: predicted {p_val:.2f} yrs, actual {a_val:.2f} yrs\")\n",
    "\n",
    "# Metrics\n",
    "pred_vals = np.array([p for _, p, _ in preds])\n",
    "actual_vals = np.array([a for _, _, a in preds])\n",
    "rmse = np.sqrt(np.mean((pred_vals - actual_vals) ** 2))\n",
    "mae = np.mean(np.abs(pred_vals - actual_vals))\n",
    "\n",
    "print(f\"\\nRoot Mean Squared Error: {rmse:.2f} years\")\n",
    "print(f\"Mean Absolute Error: {mae:.2f} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 324,
     "status": "ok",
     "timestamp": 1765150077965,
     "user": {
      "displayName": "Alexander Nguyen",
      "userId": "03900039906772620645"
     },
     "user_tz": 480
    },
    "id": "CkF0axYzhLQ8",
    "outputId": "b9282022-c568-4062-9bdc-e02fc67502a6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Data\n",
    "actual = np.array([a for _, _, a in preds])\n",
    "pred = np.array([p for _, p, _ in preds])\n",
    "\n",
    "# Compute KDE\n",
    "kde_actual = gaussian_kde(actual)\n",
    "kde_pred = gaussian_kde(pred)\n",
    "\n",
    "# Evaluate over a common grid\n",
    "xmin = min(actual.min(), pred.min())\n",
    "xmax = max(actual.max(), pred.max())\n",
    "xgrid = np.linspace(xmin, xmax, 500)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(xgrid, kde_actual(xgrid), label=\"Actual Age\", linewidth=2)\n",
    "plt.plot(xgrid, kde_pred(xgrid), label=\"Predicted Age\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Density Curves: Actual vs Predicted Age\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4425,
     "status": "ok",
     "timestamp": 1765150082395,
     "user": {
      "displayName": "Alexander Nguyen",
      "userId": "03900039906772620645"
     },
     "user_tz": 480
    },
    "id": "OD8zycWwgE-z",
    "outputId": "c06d4971-c46d-4547-e857-1f790fdf89a5"
   },
   "outputs": [],
   "source": [
    "# Attention Extraction\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "\n",
    "all_attn_matrices = {}      # key: subj_id, value: NxN attention matrix\n",
    "num_rois = dataset[0].x.shape[0]\n",
    "\n",
    "print(\"\\nExtracting attention matrices...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "\n",
    "        # Get attention weights from the first GAT layer\n",
    "        _, (edge_index, attn_weights) = model.gat1(\n",
    "            batch.x, batch.edge_index, return_attention_weights=True\n",
    "        )\n",
    "\n",
    "        # Average across attention heads\n",
    "        attn_mean = attn_weights.mean(dim=1).cpu().numpy()\n",
    "\n",
    "        # Boundaries for each graph in the batch\n",
    "        batch_graph_ptr = batch.ptr.cpu().numpy()\n",
    "        edge_index_np = edge_index.cpu().numpy()\n",
    "\n",
    "        # Loop through subjects in this batch\n",
    "        for i, subj_id in enumerate(batch.sub_id):\n",
    "\n",
    "            start = batch_graph_ptr[i]\n",
    "            end   = batch_graph_ptr[i+1]\n",
    "\n",
    "            # Extract edges belonging to this subject\n",
    "            node_mask = (edge_index_np[0] >= start) & (edge_index_np[0] < end)\n",
    "            idxs = np.where(node_mask)[0]\n",
    "\n",
    "            # Build empty N×N matrix\n",
    "            attn_mat = np.zeros((num_rois, num_rois))\n",
    "\n",
    "            # Fill matrix\n",
    "            for j in idxs:\n",
    "                u = edge_index_np[0, j] - start\n",
    "                v = edge_index_np[1, j] - start\n",
    "                attn_mat[u, v] = attn_mean[j]\n",
    "                attn_mat[v, u] = attn_mean[j]\n",
    "\n",
    "            # Store directly into dict\n",
    "            all_attn_matrices[subj_id] = attn_mat\n",
    "\n",
    "print(f\"Collected attention matrices for {len(all_attn_matrices)} subjects.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 586,
     "status": "ok",
     "timestamp": 1765150082983,
     "user": {
      "displayName": "Alexander Nguyen",
      "userId": "03900039906772620645"
     },
     "user_tz": 480
    },
    "id": "VrHSyPv6g1fg",
    "outputId": "aec6b5c6-e5e1-4f20-a4f3-511709223fc6"
   },
   "outputs": [],
   "source": [
    "# Average Attention Matrix\n",
    "attn_stack = np.stack(list(all_attn_matrices.values()), axis=0)\n",
    "\n",
    "avg_attn = np.mean(attn_stack, axis=0)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(avg_attn / (avg_attn.max() + 1e-8), cmap='hot_r', interpolation='nearest')\n",
    "plt.colorbar(label=\"Normalized Average Attention\")\n",
    "plt.title(\"Average Attention Matrix Across Test Subjects\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 19499,
     "status": "ok",
     "timestamp": 1765150102484,
     "user": {
      "displayName": "Alexander Nguyen",
      "userId": "03900039906772620645"
     },
     "user_tz": 480
    },
    "id": "wVRfDLHES2Yj",
    "outputId": "5af260e1-ed36-4978-a49f-e3056316fd7b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Attention extraction\n",
    "def extract_attention_matrices_dict(model, dataset, layer='gat1', batch_size=4, ignore_self_loops=True):\n",
    "    \"\"\"\n",
    "    Extract attention matrices for all subjects in dataset, return as dict keyed by sub_id.\n",
    "\n",
    "    Args:\n",
    "        model: Trained GAT model\n",
    "        dataset: list of PyG Data objects\n",
    "        layer: which GAT layer to extract attention from\n",
    "        batch_size: batch size for DataLoader\n",
    "        ignore_self_loops: if True, set diagonal to 0\n",
    "\n",
    "    Returns:\n",
    "        all_attn_matrices: dict {sub_id: NxN attention matrix}\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    all_attn_matrices = {}\n",
    "    num_nodes = dataset[0].x.shape[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            gat_layer = getattr(model, layer)\n",
    "            _, (edge_index, attn_weights) = gat_layer(\n",
    "                batch.x, batch.edge_index, return_attention_weights=True\n",
    "            )\n",
    "\n",
    "            # Average over heads if multi-head\n",
    "            if attn_weights.dim() > 1 and attn_weights.shape[1] > 1:\n",
    "                attn_weights = attn_weights.mean(dim=1)\n",
    "\n",
    "            edge_index_np = edge_index.cpu().numpy()\n",
    "            attn_np = attn_weights.cpu().numpy()\n",
    "            batch_ptr = batch.ptr.cpu().numpy()\n",
    "\n",
    "            for i, sub_id in enumerate(batch.sub_id):\n",
    "                start, end = batch_ptr[i], batch_ptr[i + 1]\n",
    "                mask = (edge_index_np[0] >= start) & (edge_index_np[0] < end)\n",
    "                idxs = np.where(mask)[0]\n",
    "\n",
    "                attn_mat = np.zeros((num_nodes, num_nodes))\n",
    "                for j in idxs:\n",
    "                    u = edge_index_np[0, j] - start\n",
    "                    v = edge_index_np[1, j] - start\n",
    "                    attn_mat[u, v] = attn_np[j]\n",
    "                    attn_mat[v, u] = attn_np[j]  # symmetric\n",
    "\n",
    "                if ignore_self_loops:\n",
    "                    np.fill_diagonal(attn_mat, 0)\n",
    "\n",
    "                all_attn_matrices[sub_id] = attn_mat\n",
    "\n",
    "    return all_attn_matrices\n",
    "\n",
    "# Compare attention by age\n",
    "def compare_attention_by_age_dict(dataset, all_attn_matrices, age_threshold=None):\n",
    "    \"\"\"\n",
    "    Compare attention patterns between younger and older subjects.\n",
    "    all_attn_matrices: dict {sub_id: NxN attention matrix}\n",
    "    \"\"\"\n",
    "    ages = {d.sub_id: d.y.item() for d in dataset}\n",
    "    if age_threshold is None:\n",
    "        age_threshold = np.median(list(ages.values()))\n",
    "\n",
    "    younger_subs = [sub_id for sub_id, age in ages.items() if age < age_threshold]\n",
    "    older_subs = [sub_id for sub_id, age in ages.items() if age >= age_threshold]\n",
    "\n",
    "    if len(younger_subs) == 0 or len(older_subs) == 0:\n",
    "        raise ValueError(\"No subjects in one of the age groups.\")\n",
    "\n",
    "    younger_attn = np.mean([all_attn_matrices[sub_id] for sub_id in younger_subs], axis=0)\n",
    "    older_attn = np.mean([all_attn_matrices[sub_id] for sub_id in older_subs], axis=0)\n",
    "    diff_attn = older_attn - younger_attn\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    im1 = axes[0].imshow(younger_attn, cmap='YlOrRd', interpolation='nearest')\n",
    "    axes[0].set_title(f'Younger (n={len(younger_subs)})')\n",
    "    plt.colorbar(im1, ax=axes[0], fraction=0.046)\n",
    "\n",
    "    im2 = axes[1].imshow(older_attn, cmap='YlOrRd', interpolation='nearest')\n",
    "    axes[1].set_title(f'Older (n={len(older_subs)})')\n",
    "    plt.colorbar(im2, ax=axes[1], fraction=0.046)\n",
    "\n",
    "    im3 = axes[2].imshow(diff_attn, cmap='RdBu_r', interpolation='nearest',\n",
    "                         vmin=-np.abs(diff_attn).max(),\n",
    "                         vmax=np.abs(diff_attn).max())\n",
    "    axes[2].set_title('Difference (Older - Younger)')\n",
    "    plt.colorbar(im3, ax=axes[2], fraction=0.046)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, (younger_attn, older_attn, diff_attn)\n",
    "\n",
    "all_attn_matrices = extract_attention_matrices_dict(model, dataset, layer='gat1')\n",
    "fig, (young_attn, old_attn, diff_attn) = compare_attention_by_age_dict(dataset, all_attn_matrices)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8unw1n_Xd6f1"
   },
   "source": [
    "Spectral Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109288,
     "status": "ok",
     "timestamp": 1765153325992,
     "user": {
      "displayName": "Alexander Nguyen",
      "userId": "03900039906772620645"
     },
     "user_tz": 480
    },
    "id": "G_AtfKazPaxH",
    "outputId": "d14e276f-2814-4554-9ee6-dd7560f7160c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.linalg import eigh\n",
    "import networkx as nx\n",
    "\n",
    "# Laplacian\n",
    "def laplacian_from_adj(A):\n",
    "    A = np.array(A, dtype=float)\n",
    "    np.fill_diagonal(A, 0)\n",
    "    A = np.clip(A, a_min=0, a_max=None)\n",
    "    D = np.diag(A.sum(axis=1))\n",
    "    return D - A\n",
    "\n",
    "# Spectral subnetworks\n",
    "def spectral_subnetworks_from_L(L, p=6, k_clusters=6, normalize_eigvecs=True):\n",
    "    vals, vecs = eigh(L)\n",
    "    Vp = vecs[:, :p]\n",
    "    if normalize_eigvecs:\n",
    "        Vp = Vp / (np.linalg.norm(Vp, axis=1, keepdims=True) + 1e-12)\n",
    "    kmeans = KMeans(n_clusters=k_clusters, random_state=0).fit(Vp)\n",
    "    labels = kmeans.labels_\n",
    "    return labels, vals[:p], Vp\n",
    "\n",
    "# Extract high-attention edges\n",
    "def extract_high_attention_edges(attn_mat, percentile=80):\n",
    "    \"\"\"\n",
    "    Extract top X percentile of edges by attention weight.\n",
    "    Returns edge list as set of (i,j) tuples where i < j\n",
    "\n",
    "    Args:\n",
    "        attn_mat: NxN attention matrix\n",
    "        percentile: Keep edges above this percentile (0-100)\n",
    "                   e.g., 80 = top 20% of edges\n",
    "    \"\"\"\n",
    "    N = attn_mat.shape[0]\n",
    "    mat = attn_mat.copy()\n",
    "    np.fill_diagonal(mat, 0)\n",
    "\n",
    "    # Get all non-zero attention values\n",
    "    all_weights = []\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            weight = max(mat[i, j], mat[j, i])  # Take max of both directions\n",
    "            if weight > 0:\n",
    "                all_weights.append(weight)\n",
    "\n",
    "    if len(all_weights) == 0:\n",
    "        return set()\n",
    "\n",
    "    # Compute threshold based on percentile\n",
    "    threshold = np.percentile(all_weights, percentile)\n",
    "\n",
    "    # Extract edges above threshold\n",
    "    high_attn_edges = set()\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            weight = max(mat[i, j], mat[j, i])\n",
    "            if weight >= threshold:\n",
    "                high_attn_edges.add((i, j))\n",
    "\n",
    "    return high_attn_edges\n",
    "\n",
    "# Extract spectral cluster edges\n",
    "def extract_spectral_edges(spec_labels):\n",
    "    \"\"\"\n",
    "    Extract edges within each spectral cluster.\n",
    "    Returns dict mapping cluster_id -> set of edges\n",
    "    \"\"\"\n",
    "    N = len(spec_labels)\n",
    "    spectral_edges = {}\n",
    "\n",
    "    for cluster_id in np.unique(spec_labels):\n",
    "        cluster_nodes = np.where(spec_labels == cluster_id)[0]\n",
    "        edges_in_cluster = set()\n",
    "\n",
    "        # All edges within this cluster\n",
    "        for i in cluster_nodes:\n",
    "            for j in cluster_nodes:\n",
    "                if i < j:  # Avoid duplicates\n",
    "                    edges_in_cluster.add((i, j))\n",
    "\n",
    "        spectral_edges[cluster_id] = edges_in_cluster\n",
    "\n",
    "    return spectral_edges\n",
    "\n",
    "# Find consensus subnetworks\n",
    "def find_consensus_subnetworks(high_attn_edges, spectral_edges, spec_labels):\n",
    "    \"\"\"\n",
    "    Find edges that appear in BOTH high-attention and spectral clusters.\n",
    "\n",
    "    Returns:\n",
    "        consensus_subnetworks: dict with cluster_id -> consensus info\n",
    "    \"\"\"\n",
    "    consensus_subnetworks = {}\n",
    "\n",
    "    for cluster_id, cluster_edges in spectral_edges.items():\n",
    "        # Find intersection: edges in both sets\n",
    "        consensus_edges = high_attn_edges.intersection(cluster_edges)\n",
    "\n",
    "        # Calculate overlap metrics\n",
    "        n_consensus = len(consensus_edges)\n",
    "        n_cluster = len(cluster_edges)\n",
    "\n",
    "        if n_cluster > 0:\n",
    "            overlap_ratio = n_consensus / n_cluster\n",
    "        else:\n",
    "            overlap_ratio = 0\n",
    "\n",
    "        # Get nodes in this cluster\n",
    "        cluster_nodes = np.where(spec_labels == cluster_id)[0]\n",
    "\n",
    "        # Get nodes involved in consensus edges\n",
    "        consensus_nodes = set()\n",
    "        for i, j in consensus_edges:\n",
    "            consensus_nodes.add(i)\n",
    "            consensus_nodes.add(j)\n",
    "\n",
    "        consensus_subnetworks[cluster_id] = {\n",
    "            'consensus_edges': consensus_edges,\n",
    "            'n_consensus_edges': n_consensus,\n",
    "            'n_cluster_edges': n_cluster,\n",
    "            'overlap_ratio': overlap_ratio,\n",
    "            'cluster_nodes': cluster_nodes.tolist(),\n",
    "            'consensus_nodes': list(consensus_nodes),\n",
    "            'n_consensus_nodes': len(consensus_nodes)\n",
    "        }\n",
    "\n",
    "    return consensus_subnetworks\n",
    "\n",
    "# Metrics for consensus analysis\n",
    "def compute_consensus_metrics(consensus_subnetworks, high_attn_edges):\n",
    "    \"\"\"\n",
    "    Compute aggregate metrics across all consensus subnetworks\n",
    "    \"\"\"\n",
    "    # Total consensus edges across all clusters\n",
    "    total_consensus_edges = set()\n",
    "    for data in consensus_subnetworks.values():\n",
    "        total_consensus_edges.update(data['consensus_edges'])\n",
    "\n",
    "    # What fraction of high-attention edges are in consensus?\n",
    "    consensus_coverage = len(total_consensus_edges) / len(high_attn_edges) if len(high_attn_edges) > 0 else 0\n",
    "\n",
    "    # How many clusters have meaningful consensus?\n",
    "    significant_clusters = sum(1 for data in consensus_subnetworks.values()\n",
    "                              if data['overlap_ratio'] > 0.1)  # >10% overlap\n",
    "\n",
    "    return {\n",
    "        'total_consensus_edges': len(total_consensus_edges),\n",
    "        'total_high_attn_edges': len(high_attn_edges),\n",
    "        'consensus_coverage': consensus_coverage,\n",
    "        'n_significant_clusters': significant_clusters,\n",
    "        'n_total_clusters': len(consensus_subnetworks)\n",
    "    }\n",
    "\n",
    "# Full analysis for all subjects\n",
    "def analyze_consensus_all_subjects(cov_dict, all_attn_matrices,\n",
    "                                   percentile=80, p=6, k_clusters=6):\n",
    "    \"\"\"\n",
    "    Perform consensus analysis for all subjects\n",
    "\n",
    "    Args:\n",
    "        percentile: Keep top X% of edges by attention (e.g., 80 = top 20%)\n",
    "        p: Number of eigenvectors for spectral clustering\n",
    "        k_clusters: Number of clusters\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for subj_id, cov in cov_dict.items():\n",
    "        if subj_id not in all_attn_matrices:\n",
    "            continue\n",
    "\n",
    "        attn = all_attn_matrices[subj_id]\n",
    "        L = laplacian_from_adj(cov)\n",
    "\n",
    "        # Spectral clustering\n",
    "        spec_labels, vals, vecs = spectral_subnetworks_from_L(L, p=p, k_clusters=k_clusters)\n",
    "\n",
    "        # Extract high-attention edges (PERCENTILE-BASED)\n",
    "        high_attn_edges = extract_high_attention_edges(attn, percentile=percentile)\n",
    "\n",
    "        # Extract spectral cluster edges\n",
    "        spectral_edges = extract_spectral_edges(spec_labels)\n",
    "\n",
    "        # Find consensus\n",
    "        consensus_subnetworks = find_consensus_subnetworks(\n",
    "            high_attn_edges, spectral_edges, spec_labels\n",
    "        )\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics = compute_consensus_metrics(consensus_subnetworks, high_attn_edges)\n",
    "\n",
    "        results[subj_id] = {\n",
    "            'consensus_subnetworks': consensus_subnetworks,\n",
    "            'metrics': metrics,\n",
    "            'spec_labels': spec_labels,\n",
    "            'high_attn_edges': high_attn_edges\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "# CROSS-SUBJECT CONSENSUS ANALYSIS\n",
    "def aggregate_consensus_across_subjects(consensus_results, min_subjects=0.5):\n",
    "    \"\"\"\n",
    "    Find consensus edges that appear consistently across multiple subjects.\n",
    "    \"\"\"\n",
    "    n_subjects = len(consensus_results)\n",
    "\n",
    "    if min_subjects <= 1.0:\n",
    "        min_count = int(min_subjects * n_subjects)\n",
    "    else:\n",
    "        min_count = int(min_subjects)\n",
    "\n",
    "    # Count how many subjects have each consensus edge\n",
    "    edge_counts = {}\n",
    "\n",
    "    for subj_id, result in consensus_results.items():\n",
    "        # Get all consensus edges for this subject\n",
    "        all_consensus_edges = set()\n",
    "        for cluster_data in result['consensus_subnetworks'].values():\n",
    "            all_consensus_edges.update(cluster_data['consensus_edges'])\n",
    "\n",
    "        # Count each edge\n",
    "        for edge in all_consensus_edges:\n",
    "            if edge not in edge_counts:\n",
    "                edge_counts[edge] = []\n",
    "            edge_counts[edge].append(subj_id)\n",
    "\n",
    "    # Filter edges that appear in enough subjects\n",
    "    reproducible_edges = {}\n",
    "    for edge, subjects in edge_counts.items():\n",
    "        count = len(subjects)\n",
    "        if count >= min_count:\n",
    "            reproducible_edges[edge] = {\n",
    "                'count': count,\n",
    "                'frequency': count / n_subjects,\n",
    "                'subjects': subjects\n",
    "            }\n",
    "\n",
    "    return reproducible_edges, edge_counts\n",
    "\n",
    "def find_reproducible_subnetworks(reproducible_edges, min_edge_frequency=0.3, min_cluster_size=5):\n",
    "    \"\"\"\n",
    "    Identify cohesive subnetworks formed by reproducible consensus edges.\n",
    "    \"\"\"\n",
    "    # Build graph from reproducible edges\n",
    "    G = nx.Graph()\n",
    "    for edge, data in reproducible_edges.items():\n",
    "        if data['frequency'] >= min_edge_frequency:\n",
    "            G.add_edge(edge[0], edge[1], weight=data['frequency'])\n",
    "\n",
    "    # Find connected components (cohesive subnetworks)\n",
    "    subnetworks = []\n",
    "    for component in nx.connected_components(G):\n",
    "        if len(component) >= min_cluster_size:\n",
    "            nodes = list(component)\n",
    "            edges = [(u, v) for u, v in G.edges() if u in component and v in component]\n",
    "\n",
    "            # Calculate average edge frequency\n",
    "            avg_frequency = np.mean([reproducible_edges[(min(u,v), max(u,v))]['frequency']\n",
    "                                    for u, v in edges])\n",
    "\n",
    "            subnetworks.append({\n",
    "                'nodes': sorted(nodes),\n",
    "                'n_nodes': len(nodes),\n",
    "                'edges': edges,\n",
    "                'n_edges': len(edges),\n",
    "                'avg_edge_frequency': avg_frequency,\n",
    "                'density': len(edges) / (len(nodes) * (len(nodes) - 1) / 2) if len(nodes) > 1 else 0\n",
    "            })\n",
    "\n",
    "    # Sort by number of nodes (largest first)\n",
    "    subnetworks.sort(key=lambda x: x['n_nodes'], reverse=True)\n",
    "\n",
    "    return subnetworks\n",
    "\n",
    "def summarize_cluster_consistency(consensus_results):\n",
    "    \"\"\"\n",
    "    Analyze which spectral clusters consistently have high consensus across subjects.\n",
    "    \"\"\"\n",
    "    # Track cluster metrics across all subjects\n",
    "    cluster_stats = {}\n",
    "\n",
    "    for subj_id, result in consensus_results.items():\n",
    "        for cluster_id, data in result['consensus_subnetworks'].items():\n",
    "            if cluster_id not in cluster_stats:\n",
    "                cluster_stats[cluster_id] = {\n",
    "                    'overlap_ratios': [],\n",
    "                    'n_consensus_edges': [],\n",
    "                    'n_nodes': [],\n",
    "                    'subjects_with_cluster': []\n",
    "                }\n",
    "\n",
    "            cluster_stats[cluster_id]['overlap_ratios'].append(data['overlap_ratio'])\n",
    "            cluster_stats[cluster_id]['n_consensus_edges'].append(data['n_consensus_edges'])\n",
    "            cluster_stats[cluster_id]['n_nodes'].append(len(data['cluster_nodes']))\n",
    "            cluster_stats[cluster_id]['subjects_with_cluster'].append(subj_id)\n",
    "\n",
    "    # Compute summary statistics\n",
    "    summary = []\n",
    "    for cluster_id, stats in cluster_stats.items():\n",
    "        summary.append({\n",
    "            'cluster_id': cluster_id,\n",
    "            'n_subjects': len(stats['subjects_with_cluster']),\n",
    "            'mean_overlap': np.mean(stats['overlap_ratios']),\n",
    "            'std_overlap': np.std(stats['overlap_ratios']),\n",
    "            'mean_consensus_edges': np.mean(stats['n_consensus_edges']),\n",
    "            'mean_nodes': np.mean(stats['n_nodes']),\n",
    "            'high_overlap_subjects': sum(1 for x in stats['overlap_ratios'] if x > 0.1)\n",
    "        })\n",
    "\n",
    "    # Sort by mean overlap (best performing clusters first)\n",
    "    summary.sort(key=lambda x: x['mean_overlap'], reverse=True)\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "# BRAIN REGION MAPPING FUNCTIONS\n",
    "def map_subnetworks_to_regions(subnetworks, roi_labels):\n",
    "    \"\"\"\n",
    "    Map node IDs in subnetworks to actual brain region names.\n",
    "\n",
    "    Args:\n",
    "        subnetworks: List of subnetwork dictionaries\n",
    "        roi_labels: List of ROI names (e.g., ['R_V1_ROI', 'R_MST_ROI', ...])\n",
    "\n",
    "    Returns:\n",
    "        Subnetworks with added 'region_names' field\n",
    "    \"\"\"\n",
    "    for subnet in subnetworks:\n",
    "        region_names = [roi_labels[node_id] for node_id in subnet['nodes']]\n",
    "        subnet['region_names'] = region_names\n",
    "\n",
    "    return subnetworks\n",
    "\n",
    "def print_subnetwork_regions(subnetworks, roi_labels, n_subnetworks=3):\n",
    "    \"\"\"\n",
    "    Pretty print brain regions in each subnetwork.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"BRAIN REGIONS IN REPRODUCIBLE SUBNETWORKS\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    for idx, subnet in enumerate(subnetworks[:n_subnetworks], 1):\n",
    "        print(f\"\\n--- Subnetwork {idx} ({subnet['n_nodes']} regions) ---\")\n",
    "\n",
    "        region_names = [roi_labels[node_id] for node_id in subnet['nodes']]\n",
    "\n",
    "        # Group by hemisphere if ROI names start with L_ or R_\n",
    "        left_regions = [r for r in region_names if r.startswith('L_')]\n",
    "        right_regions = [r for r in region_names if r.startswith('R_')]\n",
    "        other_regions = [r for r in region_names if not (r.startswith('L_') or r.startswith('R_'))]\n",
    "\n",
    "        if right_regions:\n",
    "            print(f\"\\nRight hemisphere ({len(right_regions)} regions):\")\n",
    "            for r in sorted(right_regions):\n",
    "                print(f\"  {r}\")\n",
    "\n",
    "        if left_regions:\n",
    "            print(f\"\\nLeft hemisphere ({len(left_regions)} regions):\")\n",
    "            for r in sorted(left_regions):\n",
    "                print(f\"  {r}\")\n",
    "\n",
    "        if other_regions:\n",
    "            print(f\"\\nOther regions ({len(other_regions)}):\")\n",
    "            for r in sorted(other_regions):\n",
    "                print(f\"  {r}\")\n",
    "\n",
    "def analyze_subnetwork_composition(subnetworks, roi_labels):\n",
    "    \"\"\"\n",
    "    Analyze what types of brain regions appear in subnetworks.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SUBNETWORK COMPOSITION ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    for idx, subnet in enumerate(subnetworks[:3], 1):\n",
    "        region_names = [roi_labels[node_id] for node_id in subnet['nodes']]\n",
    "\n",
    "        # Count region types (based on common prefixes in HCP parcellation)\n",
    "        region_types = {}\n",
    "        for region in region_names:\n",
    "            # Extract region type (e.g., V1, MST, FEF from R_V1_ROI)\n",
    "            parts = region.split('_')\n",
    "            if len(parts) >= 2:\n",
    "                region_type = parts[1]  # e.g., 'V1', 'MST', '4'\n",
    "                region_types[region_type] = region_types.get(region_type, 0) + 1\n",
    "\n",
    "        print(f\"\\nSubnetwork {idx}:\")\n",
    "        print(f\"  Total regions: {len(region_names)}\")\n",
    "        print(f\"  Unique region types: {len(region_types)}\")\n",
    "        print(f\"  Most common types:\")\n",
    "\n",
    "        # Sort by frequency\n",
    "        sorted_types = sorted(region_types.items(), key=lambda x: x[1], reverse=True)\n",
    "        for region_type, count in sorted_types[:10]:\n",
    "            print(f\"    {region_type}: {count}\")\n",
    "\n",
    "# MAIN ANALYSIS PIPELINE\n",
    "\n",
    "# k=10\n",
    "print(\"=\"*80)\n",
    "print(\"CONSENSUS SUBNETWORK ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fixed_k = 10  # Choose number of clusters\n",
    "print(f\"\\nUsing k={fixed_k} clusters\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MULTI-SCALE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "k_values = [fixed_k]\n",
    "multi_scale_results = {}\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nAnalyzing with k={k} clusters...\")\n",
    "\n",
    "    results = analyze_consensus_all_subjects(\n",
    "        cov_dict,\n",
    "        all_attn_matrices,\n",
    "        percentile=80,\n",
    "        p=min(15, k),\n",
    "        k_clusters=k\n",
    "    )\n",
    "\n",
    "    coverage_scores = [r['metrics']['consensus_coverage'] for r in results.values()]\n",
    "    sig_clusters = [r['metrics']['n_significant_clusters'] for r in results.values()]\n",
    "    consensus_edges = [r['metrics']['total_consensus_edges'] for r in results.values()]\n",
    "\n",
    "    multi_scale_results[k] = {\n",
    "        'results': results,\n",
    "        'mean_coverage': np.mean(coverage_scores),\n",
    "        'std_coverage': np.std(coverage_scores),\n",
    "        'mean_sig_clusters': np.mean(sig_clusters),\n",
    "        'mean_consensus_edges': np.mean(consensus_edges)\n",
    "    }\n",
    "\n",
    "    print(f\"  Coverage: {np.mean(coverage_scores):.3f} ± {np.std(coverage_scores):.3f}\")\n",
    "    print(f\"  Significant clusters: {np.mean(sig_clusters):.1f}\")\n",
    "    print(f\"  Consensus edges: {np.mean(consensus_edges):.1f}\")\n",
    "\n",
    "best_k = fixed_k\n",
    "consensus_results = multi_scale_results[best_k]['results']\n",
    "\n",
    "best_k = fixed_k\n",
    "consensus_results = multi_scale_results[best_k]['results']\n",
    "\n",
    "# Cross-subject reproducibility\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CROSS-SUBJECT REPRODUCIBILITY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "reproducible_edges, all_edge_counts = aggregate_consensus_across_subjects(\n",
    "    consensus_results,\n",
    "    min_subjects=0.3\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal unique consensus edges: {len(all_edge_counts)}\")\n",
    "print(f\"Reproducible edges (≥30% subjects): {len(reproducible_edges)}\")\n",
    "\n",
    "# Show most reproducible edges\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TOP 50 MOST REPRODUCIBLE CONSENSUS EDGES\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\n{'Rank':<6} {'Edge (i,j)':<20} {'Frequency':<12} {'# Subjects':<12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "sorted_edges = sorted(reproducible_edges.items(),\n",
    "                     key=lambda x: x[1]['frequency'],\n",
    "                     reverse=True)\n",
    "\n",
    "for rank, (edge, data) in enumerate(sorted_edges[:50], 1):\n",
    "    print(f\"{rank:<6} {str(edge):<20} {data['frequency']:<12.3f} {data['count']:<12}\")\n",
    "\n",
    "#  Find subnetworks\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"REPRODUCIBLE SUBNETWORKS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "subnetworks = find_reproducible_subnetworks(\n",
    "    reproducible_edges,\n",
    "    min_edge_frequency=0.3,\n",
    "    min_cluster_size=5\n",
    ")\n",
    "\n",
    "print(f\"\\nFound {len(subnetworks)} reproducible subnetworks\\n\")\n",
    "\n",
    "for idx, subnet in enumerate(subnetworks[:5]):\n",
    "    print(f\"Subnetwork {idx+1}:\")\n",
    "    print(f\"  Nodes: {subnet['n_nodes']} nodes\")\n",
    "    print(f\"  Edges: {subnet['n_edges']} edges\")\n",
    "    print(f\"  Avg edge frequency: {subnet['avg_edge_frequency']:.3f}\")\n",
    "    print(f\"  Density: {subnet['density']:.3f}\")\n",
    "    print(f\"  Node IDs: {subnet['nodes'][:15]}{'...' if len(subnet['nodes']) > 15 else ''}\")\n",
    "    print()\n",
    "\n",
    "# Show edges in each subnetwork\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EDGES IN EACH REPRODUCIBLE SUBNETWORK\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "for idx, subnet in enumerate(subnetworks[:3], 1):\n",
    "    print(f\"\\n--- Subnetwork {idx} ({subnet['n_nodes']} nodes, {subnet['n_edges']} edges) ---\")\n",
    "\n",
    "    # Get edges with frequencies\n",
    "    subnet_edges_with_freq = []\n",
    "    for edge in subnet['edges']:\n",
    "        edge_tuple = (min(edge[0], edge[1]), max(edge[0], edge[1]))\n",
    "        if edge_tuple in reproducible_edges:\n",
    "            freq = reproducible_edges[edge_tuple]['frequency']\n",
    "            subnet_edges_with_freq.append((edge_tuple, freq))\n",
    "\n",
    "    # Sort by frequency\n",
    "    subnet_edges_with_freq.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Show top 20 or all\n",
    "    n_show = min(20, len(subnet_edges_with_freq))\n",
    "    print(f\"\\nTop {n_show} edges by frequency:\")\n",
    "    print(f\"{'Edge':<20} {'Frequency':<12}\")\n",
    "    print(\"-\"*40)\n",
    "\n",
    "    for edge, freq in subnet_edges_with_freq[:n_show]:\n",
    "        print(f\"{str(edge):<20} {freq:<12.3f}\")\n",
    "\n",
    "    if len(subnet_edges_with_freq) > n_show:\n",
    "        print(f\"... and {len(subnet_edges_with_freq) - n_show} more edges\")\n",
    "\n",
    "# Cluster consistency\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SPECTRAL CLUSTER CONSISTENCY\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "cluster_summary = summarize_cluster_consistency(consensus_results)\n",
    "\n",
    "print(f\"\\n{'Cluster':<10} {'N Subj':<10} {'Mean Overlap':<15} {'High Overlap':<15} {'Mean Edges':<12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for stats in cluster_summary[:15]:\n",
    "    print(f\"{stats['cluster_id']:<10} {stats['n_subjects']:<10} \"\n",
    "          f\"{stats['mean_overlap']:<15.3f} {stats['high_overlap_subjects']:<15} \"\n",
    "          f\"{stats['mean_consensus_edges']:<12.1f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1765150210394,
     "user": {
      "displayName": "Alexander Nguyen",
      "userId": "03900039906772620645"
     },
     "user_tz": 480
    },
    "id": "3nn0f3kqgPVw",
    "outputId": "042f68e0-1f0c-4a0c-c4c4-6d7f3919a904"
   },
   "outputs": [],
   "source": [
    "# Map to brain regions\n",
    "\n",
    "# Extract ROI labels from one of your covariance CSV files\n",
    "def load_roi_labels(csv_path):\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "\n",
    "    # Get labels from first row, skip the first cell (which is usually empty or \"ROI\")\n",
    "    roi_labels = df.iloc[0, 1:].tolist()\n",
    "\n",
    "    # Clean up labels (remove any extra whitespace, NaN, etc.)\n",
    "    roi_labels = [str(label).strip() for label in roi_labels if pd.notna(label)]\n",
    "\n",
    "    return roi_labels\n",
    "\n",
    "# Load ROI labels from the first subject's file\n",
    "first_subject_file = os.path.join(cov_dir, f\"{first_id}.csv\")\n",
    "roi_labels = load_roi_labels(first_subject_file)\n",
    "\n",
    "print(f\"Extracted {len(roi_labels)} ROI labels\")\n",
    "print(f\"First 10 labels: {roi_labels[:10]}\")\n",
    "print(f\"Last 10 labels: {roi_labels[-10:]}\")\n",
    "\n",
    "# Print regions in each subnetwork\n",
    "print_subnetwork_regions(subnetworks, roi_labels, n_subnetworks=3)\n",
    "\n",
    "# Analyze composition\n",
    "analyze_subnetwork_composition(subnetworks, roi_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1765150646643,
     "user": {
      "displayName": "Alexander Nguyen",
      "userId": "03900039906772620645"
     },
     "user_tz": 480
    },
    "id": "iluHP174gp-a",
    "outputId": "6b3b036f-441a-4736-e8dc-bb5017e98bba"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Load and normalize brain region CSV\n",
    "def load_brain_regions(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    region_map = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        key = row['regionName']\n",
    "\n",
    "        # Replace '-' with '_'\n",
    "        key = key.replace('-', '_')\n",
    "\n",
    "        # Move hemisphere to front if at the end\n",
    "        if key.endswith('_L'):\n",
    "            key = 'L_' + key[:-2]\n",
    "        elif key.endswith('_R'):\n",
    "            key = 'R_' + key[:-2]\n",
    "\n",
    "        region_map[key] = {\n",
    "            'regionLongName': row['regionLongName'],\n",
    "            'cortex': row['cortex']\n",
    "        }\n",
    "\n",
    "    return region_map\n",
    "\n",
    "# Map subnetwork nodes to region names + cortex\n",
    "def map_subnetworks_with_labels(subnetworks, roi_labels, region_map):\n",
    "    mapped_subnetworks = []\n",
    "\n",
    "    for subnet in subnetworks:\n",
    "        region_names = []\n",
    "        cortex_names = []\n",
    "        for idx in subnet['nodes']:\n",
    "            label = roi_labels[idx]\n",
    "\n",
    "            # Remove _ROI from ROI label\n",
    "            label_fixed = label.removesuffix('_ROI')\n",
    "\n",
    "            label_fixed = re.sub(r'(?<=\\d)-(?=\\d)', '_', label_fixed)\n",
    "\n",
    "            # Search in normalized CSV region map\n",
    "            matched = None\n",
    "            cortex = \"Unknown\"\n",
    "            for key in region_map:\n",
    "                if label_fixed in key:\n",
    "                    matched = region_map[key]['regionLongName']\n",
    "                    cortex = region_map[key]['cortex']\n",
    "                    break\n",
    "            if matched is None:\n",
    "                matched = f\"Unknown_{label_fixed}\"\n",
    "\n",
    "            region_names.append(matched)\n",
    "            cortex_names.append(cortex)\n",
    "\n",
    "        new_subnet = subnet.copy()\n",
    "        new_subnet['region_names'] = region_names\n",
    "        new_subnet['cortex_names'] = cortex_names\n",
    "        mapped_subnetworks.append(new_subnet)\n",
    "\n",
    "    return mapped_subnetworks\n",
    "\n",
    "csv_path = \"/content/drive/Shareddrives/291A_Brain_Connectomes/HCP-MMP1_UniqueRegionList.csv\"\n",
    "region_map = load_brain_regions(csv_path)\n",
    "\n",
    "mapped_subnetworks = map_subnetworks_with_labels(subnetworks, roi_labels, region_map)\n",
    "\n",
    "# Print subnetworks\n",
    "for idx, subnet in enumerate(mapped_subnetworks):\n",
    "    print(f\"\\nSubnetwork {idx+1} ({subnet['n_nodes']} nodes):\")\n",
    "    print(f\"{'Region':<35} {'Cortex'}\")\n",
    "    print(\"-\" * 50)\n",
    "    for region, cortex in zip(subnet['region_names'], subnet['cortex_names']):\n",
    "        print(f\"{region:<35} {cortex}\")\n",
    "\n",
    "    # Compute and print cortex frequency\n",
    "    cortex_counter = Counter(subnet['cortex_names'])\n",
    "    print(\"\\nCortex frequencies:\")\n",
    "    for cortex, count in sorted(cortex_counter.items(), key=lambda x: x[1], reverse=True):\n",
    "        pct = 100 * count / len(subnet['cortex_names'])\n",
    "        print(f\"{cortex:<45} {count} ({pct:.1f}%)\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
